#!/bin/bash
# Daily job to download logs and generate reports
. "$(dirname "$0")"/setup-vars

# Number of hours of logs to bring in on every run
# This must be larger than the time between cron runs to avoid missing logs,
# and ideally more than the time between two runs in case one is skipped.
readonly HOURS=18

# Number of hours for statistics jobs that take too much memory to run for
# the full 90 days
readonly REDUCEDSTATSHOURS=504  # 504 is 21 days

# Log lines (may) begin with <level>
readonly LOGCMD='systemd-cat --level-prefix=true --priority=warning'
# Log lines do not begin with <level>
readonly LOGCMDNOLVL='systemd-cat --level-prefix=false --priority=warning'
# First argument is an identifier; this is used in a pipe where the command can't be identified
readonly LOGCMDID='systemd-cat --level-prefix=true --priority=warning --identifier'

# Enable syslog prefixes on logs, and optionally the verbosity level
#readonly VERBOSITY=--debug --level-prefix
readonly VERBOSITY="--verbose --level-prefix"

# Enable dry-run mode on those programs that offer it
#readonly DRY=--dry-run
readonly DRY=

# Directory holding public summary reports
readonly REPORTROOT="$HOME/ROOT/static/reports"

$LOGCMDNOLVL echo Starting daily update

# Look for an old job still running.
# It's really slow if it is, but give it a chance to finish.
readonly LOCKFILE="$XDG_RUNTIME_DIR"/tc-update.lock

if [[ -e "$LOCKFILE" ]]; then
    $LOGCMDNOLVL echo "Update lock already exists (PID $(cat "$LOCKFILE"))"
    $LOGCMDNOLVL echo -n Lock created:
    $LOGCMDNOLVL ls -l "$LOCKFILE"
    if ps -p "$(cat "$LOCKFILE")" > /dev/null; then
        $LOGCMDNOLVL echo "Aborting daily update"
        exit 1
    fi
    $LOGCMDNOLVL echo Deleting stale lock
    # No need to actually delete anything; the lock will be overwritten instead.
    # Note that there is still a brief race condition here, but it shouldn't cause any issue in
    # reality because this script is run (and the lock checked) only every few hours.
fi
echo $$ > "$LOCKFILE"

# Download logs
$LOGCMD tcingestlog $VERBOSITY $DRY --origin=appveyor --account=curlorg --howrecent $HOURS
$LOGCMD tcingestlog $VERBOSITY $DRY --origin=azure --account daniel0244 --howrecent $HOURS
$LOGCMD tcingestlog $VERBOSITY $DRY --origin=cirrus --howrecent $HOURS
$LOGCMD tcingestlog $VERBOSITY $DRY --origin=circle --howrecent $HOURS
$LOGCMD tcingestlog $VERBOSITY $DRY --origin=curlauto --howrecent $HOURS
if [[ -r "$XDG_DATA_HOME/auth/ghatoken" ]]; then
    $LOGCMD tcingestlog $VERBOSITY $DRY --origin=gha --authfile "$XDG_DATA_HOME/auth/ghatoken" --howrecent $HOURS
else
    $LOGCMDNOLVL echo Skipping GHA ingestion due to no available token
fi

# The following must be done at least once a day, regardless of $HOURS
# The Time zone used should ensure that that the daily build has been created for the day already
readonly CURLFILE=$(curl -LsSf --max-time 999 --compressed https://curl.se/snapshots/ | sed -n -Ee '/NEWEST tar.xz/s/^.*(curl-[-.[:alnum:]]+\.tar\.xz).*$/\1/p')
readonly CURLDAILYPATH="$XDG_CACHE_HOME/curldaily/$CURLFILE"
if [[ -z "$CURLFILE" ]]; then
    $LOGCMDNOLVL echo Error determining daily curl
else
    if [[ -f "$CURLDAILYPATH" ]]; then
        $LOGCMDNOLVL echo "Daily build source file $CURLFILE is already downloaded!"
    else
        # --retry-connrefused should be added if the curl version supports it
        $LOGCMDNOLVL curl -o "$CURLDAILYPATH" -LsSf --max-time 999 --retry 6 --xattr "https://curl.se/snapshots/$CURLFILE"
    fi
fi

# The following must be done at least once a day (well, once every two days due to the next section)
if [[ -f "$CURLDAILYPATH" ]]; then
    $LOGCMD tcaugmentcurldaily $VERBOSITY $DRY "$CURLDAILYPATH"
else
    $LOGCMDNOLVL echo "Daily curl build file $CURLFILE could not be downloaded!"
fi

# Augment yesterday's daily builds, too, because it can be many hours before some autobuilds become
# available and we want to augment them even if they show up after the new daily build is ready.
# Use 1 hour past UTC as the time zone to give a run starting within an hour after midnight an
# extra hour to do something useful, since otherwise it will almost certainly have the same date
# as the daily file we just checked above.
readonly YESTERDAY="$(TZ=UTC+1 date -d yesterday +'%Y%m%d')"
if ls "$XDG_CACHE_HOME/curldaily/"*-"$YESTERDAY"* >/dev/null 2>&1; then
    $LOGCMD tcaugmentcurldaily $VERBOSITY $DRY "$XDG_CACHE_HOME/curldaily/"*-"$YESTERDAY"*
fi

# Update the git checkout
# git 1.8.3 doesn't support -C to select the repository to use
$LOGCMDNOLVL env GIT_DIR="$XDG_DATA_HOME/curl.git" git fetch origin master:master
# Since this is done entirely locally, always ingest a lot more than we need to to reduce
# the chance of a commit being missed.
readonly GITHOURS=$((HOURS * 50))
$LOGCMD tcgitcommitinfo $VERBOSITY $DRY "$XDG_DATA_HOME/curl.git" "$GITHOURS hours ago"
# If this shows an error, use commitchainrev to start at the given commit and use the last
# "commit" in the list as the last parameter in this command:
if ! $LOGCMDNOLVL tcdbutil checkcommitchain https://github.com/curl/curl master ff67da58c4add27b05d237533d1cb39fb3894113; then
    tcdbutil commitchainrev https://github.com/curl/curl master | tail -8 | $LOGCMDID tcdbutil
fi

$LOGCMD tcaugmentgithash $VERBOSITY $DRY

# Generate reports

REPORTTMP="$(mktemp "$REPORTROOT/tmp.XXXXXXXXXX")"
tcmetadatastats $VERBOSITY --html --report=metadata_values 2>&1 > "$REPORTTMP" | $LOGCMDID tcmetadatastats
mv "$REPORTTMP" "$REPORTROOT"/metadata-summary.html

REPORTTMP="$(mktemp "$REPORTROOT/tmp.XXXXXXXXXX")"
tcmetadatastats $VERBOSITY --html --report=test_run_stats --howrecent="$REDUCEDSTATSHOURS" 2>&1 > "$REPORTTMP" | $LOGCMDID tcmetadatastats
mv "$REPORTTMP" "$REPORTROOT"/test-stats.html

REPORTTMP="$(mktemp "$REPORTROOT/tmp.XXXXXXXXXX")"
tcmetadatastats $VERBOSITY --html --report=test_results_count --howrecent="$REDUCEDSTATSHOURS" 2>&1 > "$REPORTTMP" | $LOGCMDID tcmetadatastats
mv "$REPORTTMP" "$REPORTROOT"/results-count.html

REPORTTMP="$(mktemp "$REPORTROOT/tmp.XXXXXXXXXX")"
tcanalysissum $VERBOSITY --html 2>&1 > "$REPORTTMP" | $LOGCMDID tcanalysissum
mv "$REPORTTMP" "$REPORTROOT"/summary.html

# Delete run lock file at end
rm -f "$LOCKFILE"

$LOGCMDNOLVL echo Completed daily update
